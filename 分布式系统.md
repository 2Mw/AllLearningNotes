# 分布式系统

[TOC]

课程链接：

* [6.824 Schedule: Spring 2020 ](http://nil.csail.mit.edu/6.824/2020/schedule.html)
* [6.824 Schedule: Spring 2021](http://nil.csail.mit.edu/6.824/2021/schedule.html)，2021 添加了 lab 2D，设置更合理

推荐翻译：

* [Simviso MIT6.824 翻译](https://www.simtoco.com/#/albums?id=1000019)
* [MIT6.824 GItbook 翻译](https://mit-public-courses-cn-translatio.gitbook.io/mit6-824) 对应 Github [huihongxiao/MIT6.824](https://github.com/huihongxiao/MIT6.824)

## 零. 简介

分布式系统的核心就是通过网络来使一群计算机互相通信来完成一系列连贯的任务。

在尝试构建分布式系统之前，可以尝试使用其它方法来进行解决，因为分布式系统很难。

为什么需要构建分布式系统：

* 并行化处理，大量 CPU、内存、硬盘可以一起进行工作
* 容错处理，如果一台发生故障，可以切换到另一台计算机
* 物理原因
* 安全原因，将事情隔离多个事件来共同计算

分布式系统为什么难：

* 需要进行并发处理
* 可能遇到部分机器故障
* 性能问题

> 在看课之前必须读过整篇论文，否则看课没什么意义。

## 一. MapReduce

MapReduce: 用户使用 `map` 函数根据键值对产生一系列的**过渡键值对**(intermediate key/value pairs)，然后根据 `reduce` 函数将具有相同过渡键(intermediate key)的过渡值(intermediate value)合并。

由于数据量十分庞大，只有在数百个或者数千个机器上进行计算才能在合理的时间内完成，然而中间产生的问题比如如何并行计算、分布存储数据、错误处理，都让本来简单的算法在分布式系统上需要很复杂的代码才能实现，并且代码过于晦涩难懂，因此产生了 MapReduce。

MapReduce 使用**函数式模型**(Functional Model)，隐藏了棘手的并行化、容错、数据分布以及负载均衡的细节，可以轻松地进行大规模并行计算，并且使用**再执行**(re-execution)作为原语机制进行容错。

### 1. 编程模型

计算就是根据输入的键值对来生成输出的键值对，MapReduce 有两个关键的函数即 `Map` 和 `Reduce`。假设 MapReduce 用来计数一堆文档中字母出现的次数。

* `Map` 函数，由用户进行编写，用于将输入键值对产生过渡键值对，MapReduce 库将具有相同过渡键的过渡值传递给 `Reduce` 函数。以计数为例，Map 函数伪代码可以写为：

  ```python
  def map(key, value):
      # key 文档名，value 文档内容
      for word in value:
          EmitIntermediate(word, "1");
  ```

  `Map` 函数所作工作相当于：`(k1, v1) -> list(k2, v2)`

* `Reduce` 函数，也是由用户进行编写，用于接收一个过渡键以及对应的一系列过渡值，然后将这些过渡值合并。通常情况下，每次 `Reduce` 函数调用返回 0 或 1 个值。以计数为例，Reduce 函数伪代码可以写为：

  ```python
  def reduce(key, values):
      # key 字母，values 表示 key 对应所有的过渡值
      res = 0
      for value in values:
          res += int(value)
      Emit(str(res))
  ```

  `Reduce` 函数所作工作相当于：`(k2, list(v2)) -> newvalue`

### 2. 执行细节

![image-20221005105038103](分布式系统.assets/image-20221005105038103.png)

<center><i>图1-1</i></center>

图1-1展示了 MapReduce 的整体流程图，当用户进程调用 MapReduce 的时：

1. 用户进程中的 MapReduce 库首先将输入文件划分为 $M$ 个 16 ~ 64 MB 的小文件，并且用户进程 fork 出众多子进程。
2. 其中 Master 进程是最特殊的，用来为其他子进程分配任务。划分为 $M$ 个小文件就有 $M$ 个 `Map` 任务，并且假设有 $R$ 个 `Reduce` 任务。Master 随机选取其中一个空闲 worker 来进行分配任务。
3. Map worker 从小文件中读取内容，从文件中解析键值对并且将其传给用户定义的 `Map` 函数，将 `Map` 函数产生的过渡键值对缓冲在内存中。
4. Map worker 会定期将缓冲的键值对写入本地磁盘，然后使用分区函数（比如 $hash(key)\%R$ ）将其分为 $R$ 份用于 reduce worker 进行处理。写入本地磁盘后将包含 $R$ 个临时文件的信息传递回 Master，然后 Master 将 `Map` 好的文件位置转发给 reduce worker。
5. 当 master 通知 reduce worker 过渡键值对的存储位置之后，reduce worker 通过 RPC 来读取数据。当 reduce worker 读取完所有过渡键值对会对其进行排序，这样相同 key 的数据就可以分组在一起。这里的排序工作是必要的，因为其他的键也会分配到相同的 reduce task 中，如果所需排序数据过于庞大，也可以使用外部排序。
6. reduce worker 为每个过渡键遍历所有过渡值，然后将 key 和所有的 values 都传递给用户定义的 `Reduce` 函数，然后将结果输出到这个 reduce task 分区中。
7. 当所有 map 和 reduce worker 结束之后，master 唤醒用户进程。

最终 reduce worker 产生了 $R$ 个输出文件，一般情况下用户不需要将这 $R$ 个输出文件合并，而是转入下一个 MapReduce 任务或者其他的分布式任务中。

🔵Master 的数据结构：

在 MapReduce 任务中，Master 起到很重要的枢纽作用，并且维护了几种数据结构。

* 对于每个 map 或者 reduce 任务都有三种状态：idle, in-process, completed. 

* 对于每个 worker 还应该有身份标识。
* 对于已完成的 map 任务，master 还应该保存 $R$ 个过渡键值对数据的位置和大小，然后将这些任务将给正在运行的 reduce worker。

### 3. 容错处理

容错处理看重的能力：

* 可用性(Availablity)：当遇到故障的时候，系统会继续运行并且提供完好的服务。
* 可恢复性(recoverablity)：当发生故障的时候，系统会对其自行进行修复并且继续运行。比如可能需要保存 checkpoint 落盘。

由于 MapReduce 库是为成百上千个服务器处理大规模数据而设计的，因此容错能力必须好。在 MapReduce 任务中，可能出现 worker 故障和 master 故障。

* Master 故障

  由于 master 节点只有一个，==因此不太可能发生故障[^Q1]==，如果 master 故障 MR 任务会直接终止运算。master 节点也会定期向磁盘写入以上提到 master 数据结构信息的 checkpoint，因此当 master 故障恢复的时候可以从上一次状态继续运行。客户端如果遇到这种情况可以重新提交 MR 任务。

* Worker 故障

  Master 会定期向 worker 发送心跳包，如果在特定时间内 worker 未进行回应，master 会将 worker 标记为已故障。

  在 worker 上成功完成 map 或者 reduce 任务时应该重置为 idle 状态，worker 任务失败的时候也应该被重置为 idle 状态，方便进行重新调度。

  在 Map worker 上已完成的任务需要重新执行，因为 map 任务的结果存储在故障机器的硬盘上并且不可访问了，因此需要重新执行；==在 reduce worker 上已完成的任务不需要重新执行，因为其结果就保存在全局文件系统上[^Q2]==。

  如果一个集群中很多机器由于网络维护全部失联，MapReduce Master 会重新执行调度这些集群处理的任务，直到任务完成。

* 出现故障时候的处理机制

  在确定 map 和 reduce 函数都是确定性函数的时候，在不发生任何故障的情况下函数的在不同机器上的分布式实现产生的结果都是一致的。MapReduce 将 `Map` 和 `Reduce` 函数定义为原子性的，如果 master 重复收到已完成的 Map 任务信息，master 会选择忽略；否则 master 会将 $R$ 个文件的信息存放在其数据结构中。

> 还有一点需要注意的是，谷歌的 MR 任务是跑在其 Google File System 的基础上，当对于一堆大型文件进行 MR 任务处理的时候，在 GFS 各个机器上已经有文件的备份，因此无需占用额外的网络带宽。

如果 MR 任务中极个别 worker 上可能由于机器原因（网络带宽、CPU 占用过高、读写能力较差）导致整体任务执行时间变长，当 MR 任务进入末尾阶段，master 会将还在运行的任务进行备份执行（即在其他机器也运行），谷歌工程师发现如果开启备份执行会有效提高整体任务执行的速度。

### 4. 功能扩展

🔵 Combiner 函数

在单词计数例子中，可能会出现很多 <the, 1> 的情况，而这一个 map 任务就会传到单独的 reduce 任务中，对于在文章中 the 单词数量较多的情况，用户可以实现 `Combiner` 函数进行合并。

`Combiner` 函数在 map 任务阶段运行，不过通常阶段下 `Combiner` 函数和 `Reduce` 函数大部分实现是一致的，最主要的区别就在于输出部分， `Combiner` 函数输出在过渡文件中， `Reduce` 函数输出在最终输出文件中。

Combiner 函数会显著提高特定 MapReduce 任务。

🔵 跳过坏记录

对于某些无法产生结果的记录，可以选择忽略。因为有可能是第三方库的问题，也有可能是系统段错误等问题造成的，有时候业务允许的情况下忽略一些记录也是可以的。

🔵 其他功能

在 Map 函数或者 Reduce 函数中可以添加其他功能比如计数功能、日志功能等，使用单独的 worker 定期传给 master。（这里需要注意的是，其他功能有可能会被备份调度(backup task)的时候重执行）

## 二. GFS

![image-20221020204645705](分布式系统.assets/image-20221020204645705.png)

Google File System 是为大规模分布式数据密集型应用而涉及的可伸缩式的分布式文件系统。它与之前所有的分布式文件系统有以下 4 点不同：

* GFS 将组件故障当作一个常规事件而不是异常事件。对于数以百计并且参差不齐的硬件设施来说，无法保证这些硬件在任何事件都能够正常运行。因此常规监控、错误检测、故障容错以及自动恢复对于一个系统来说是十分重要的。
* 在通常情况下，文件是十分巨大的。
* 大部分文件修改方式是用过添加的方式而不是在源文件基础上进行修改。一旦对文件写入完毕，文件就变成只读的形式。
* 应用程序和文件系统 API 的协同涉及提高了整个系统的灵活性。GFS 对于强一致性的要求不高，因此减轻了文件系统对应用程序的苛刻要求，大大简化的 GFS 的涉及。

### 1. 设计总览

GFS 的设计前提：

* GFS 假设系统运行的硬件都是容易损坏的，因此 GFS 必须提供监控、检测、容灾以及自我恢复的功能。
* 只有两种类型的读：大规模流式读取和小规模随机读取。
* 支持大规模顺序写和小规模随机写。但是随机写不保证其运行效率
* 支持多客户的并发写入，在客户端无需实现同步机制。
* 网络的高带宽优先级要高于网络低延迟

GFS 支持普通的文件操作和额外的快照功能(snapshot)和并发记录追加(record append)的功能。

**GFS 的架构**：一个 GFS 集群是有**单个** master 节点和多个 chunkserver 组成。机器上每个文件都被分为固定大小的块(chunks)，默认64MB，每个块都由 master 分配一个唯一而且不可变的 64 位 ID 组成。为了保险起见，每个块都要在多个 chunkserver 上进行备份。客户端在获取文件的时候首先从 master 节点查看 chunkserver 的元信息(metadata)，然后根据元信息取 chunkserver 存取文件。

**单 master 节点**：采用单 master 节点的设计方式可以大大简化设计方案，并且由于大部分读写操作不在 master 节点上进行，所有单 master 节点不会成为性能瓶颈。并且设计 GFS 的初衷就是最小化与 master 节点的交互操作。

**元数据(metadata)**：在 master 节点主要存储三种类型元信息（文件和块的命名空间，文件到块的映射，块其他备份的位置），所有这些元信息都保存在 master 节点的内存中。并且元数据占用的内存空间小，不必担心会因为 chunk 过多导致 master 节点的内存不够用（2003 年）。

**操作日志**：对于 GFS 来说，操作日志十分重要。当响应客户端请求之前，必须将对应日志进行持久化存储到本地和远程，才能够返回响应操作。当日志的长度到达一定长度的时候，master 节点会生成快照，然后可以记录新的日志文件。

### 2. 一致性模型

GFS 采用的是一种较为宽松的一致性模型，在保证系统运行的同时也能较为简单和高效。GFS 在实现分布式系统宽松的一致性模型基础上，使用三个方法保证系统设计简答的特点：

1. 写数据主要依靠追加的方式而不是覆写，因为追加的方式要比随机写的方式更高效和更有弹性
2. 写时自我验证机制。由于 GFS 实现的是较为宽松的一致性模型，采用的是至少添加一次(append-at-least-once)的追加方案，因此可能会出现数据冗余重复的情况，因此需要实现数据块验证和识别的方法来识别重复冗余的数据块。
3. 自我识别机制，作用同2.

### 3. 数据交互细节

这一节主要介绍客户端、master、chunkserver如何实现数据修改、原子追加以及快照功能。

<h4>3.1. 租约(lease)和修改顺序</h4>

租约的设计初衷就是为了降低 master 节点的管理开销。由于每一次修改操作都会在多个备份服务器上执行，因此在多个备份服务器上使用租约来维护一致性修改顺序。

master 节点会授权租约给一个 chunkserver，这个 chunkserver 被称为 primary replica 即主备份服务器，和这个 chunkserver 关联的其他备份服务器都附属于主备份服务器，所有副备份服务器的修改操作都由主备份服务器的指令进行分发。通常情况下租约时长为 60 秒，每次通过心跳检测就可以延长租约。具体流程如图：

![image-20221026221922555](分布式系统.assets/image-20221026221922555.png)

1. 客户端首先询问 master 哪些 chunkserver 拥有对应数据块的租约和其他备份服务器的位置。如果没有服务器拥有租约，则 master 指定一个。
2. master 告诉客户端主副备份服务器的位置。
3. 客户端将数据发送给所有主副备份服务器。客户端可以根据网络拓扑来寻找最近的服务器，而不是直接推给主备份服务器。
4. 一旦所有的备份服务器都接收到数据，客户端像主备份服务器发送**写请求**。主备份服务器分配连续的序列号给所有修改过的块，并且以序列号顺序写入本地状态。
5. 主备份服务器转发写请求给所有副备份服务器，副备份服务器也以序列号的顺序写入本地状态。
6. 写入本地状态完毕后，副备份服务器回复 primary 已完成所有操作。
7. primary 返回给客户端。如果期间发生错误，即写入 primary 成功但是写入副备份服务器失败，就认为客户端的写入请求失败，需要重复请求。



## Appendix

[^Q1]: 为什么不太可能发生故障
[^Q2]: reduce worker 是执行在哪个机器上？