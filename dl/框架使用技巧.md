# 深度学习框架使用技巧

## Pandas

🔵`read_csv()`

读取csv文件，默认参数`header=0`，读取表格第一行为表头。 

🔵`iloc[]`和`loc[]`

`iloc[]`是根据索引来对数据进行获取，`loc[]`是根据列名来对数据进行获取。

🔵填充`NAN`值

```python
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
```

🔵`apply()`函数

参考：[pandas.DataFrame.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html)

对数据的每一行或者每一列使用对应的函数，如下这个例子是对每一列都进行归一化。

```python
a=pd.DataFrame([[1,2], [3,4]])
a.apply(lambda x:(x-x.mean)/x.std())
```

🔵`get_dummies()`函数

将分类的数据中转为列，相当于将分类的数据转为独热编码。

```python
s = pd.Series(list('abca'))
pd.get_dummies(s)
```

输出：

```
  a  b  c
0  1  0  0
1  0  1  0
2  0  0  1
3  1  0  0
```

🔵数组排序

`argsort()`是用来得到排序好的原来数组index

在一维情况下，使用`a[a.argsort()]`就得到了排序好的数组。

```python
a=np.array([4,3,1,2])
a=a[a.argsort()]
```

## Numpy

🔵广播

广播对不同形状(shape)的数组进行数值计算的方式， 对数组的算术运算通常在相应的元素上进行。

* 两个矩阵形状相同，就是对应位置的相乘

  ```python
  a = np.array([1,2,3,4]) 
  b = np.array([10,20,30,40]) 
  print (a*b) // [10,40,90,160]
  ```

* 两矩阵形状不同

  加入a的shape为`(s1,s2,s3,s4...,sn)`,b不为标量且其shape为`(x1,x2,x3)`，b的维度小于a且有$x_1=s_{n-2},x_2=s_{n-1},x_3=s_{n}$之类，则可以进行广播。

🔵节约内存

```python
Y = Y + X		# (X)
Y[:] = Y + X	# (√)
```

## PyTorch

🔵数据集处理：

`torch.utils.data.Dataset`

Dataset分为两种：1. map-style dataset 2. iterable-style dataset

常用的是map-style dataset

映射形式的数据集的类需要实现`__getitem__()` 和 `__len__()`方法来通过index进行数据访问和查看数据集的长度。

For example, such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label from a folder on the disk.

模板：

```python
class ImgDataset(dataset.Dataset):  # 继承父类dataset
    def __init__(self, x, y=None, transform=None):
        self.x = x
        self.y = y
        if y is not None:
            self.y = tc.LongTensor(y)  # y 应该为Long Tensor
        self.transform = transform

    def __len__(self):  # 实现函数__len__
        return len(self.x)

    def __getitem__(self, index):  # 实现函数__getitem__
        X = self.x[index]
        if self.transform is not None:
            X = self.transform(X)
        if self.y is not None:
            return X, self.y[index]
        else:
            return X
```

`torch.utils.data.DataLoader` 是pytorch数据加载中最实用的包，能够以迭代的形式加载数据集。

其可以：

* 映射以及迭代的方式加载数据集（map-style and iterable-style）
* 自定义数据加载方式
* 自动分批（batch）
* 单线程或者多线程加载数据
* …………

🔵`nn.Module`的使用：

用于搭建网络模型：

```python
class Classifier(nn.Module):    # 继承父类nn.Module
    def __init__(self):
        super(Classifier, self).__init__()
        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        # torch.nn.MaxPool2d(kernel_size, stride, padding)
        # 原图输入维度 (128, 128, 3)
        self.cnn = nn.Sequential(
            # 输入为彩色图片，输出通道64 ，3X3的卷积核，1步长， 填充长度为1=>为了凑128，不然126不容易解决
            nn.Conv2d(3, 64, 3, 1, 1),  # (128, 128, 64)
            nn.BatchNorm2d(64), # norm2d的输入为特征数量，并进行标准化，这里可以设置momentum和eps
            nn.ReLU(),  # 进行ReLU函数转换
            nn.MaxPool2d(2, 2, 0),  # (64, 64, 64)

            # 上一个conv + maxpool的输出为这次的输入
            """
            	...
           	 """

            nn.Conv2d(512, 512, 3, 1, 1),  # (8, 8, 512)
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),  # (4, 4, 512)
        )

        self.fc = nn.Sequential(
            nn.Linear(512 * 4 * 4, 1024),
            nn.Dropout(0.5),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.Dropout(0.5),
            nn.ReLU(),
            nn.Linear(512, 11)
        )

    def forward(self, x):
        out = self.cnn(x) # 进行CNN转换
        out = out.view(out.size()[0], -1)   # 总感觉这句话没什么用
        return self.fc(out) # 输入网络，有四层网络：输入层，隐藏层1，隐藏层2，输出层。最后输出11维度的向量（有11种类别的食品）。
```

🔵随机种子

参考：

* [torch.backends.cudnn.benchmark](https://zhuanlan.zhihu.com/p/73711222)
* [torch.backends.cudnn.deterministic](https://zhuanlan.zhihu.com/p/141063432)

`torch.backends.cudnn.deterministic`将这个 flag 置为`True`的话，每次返回的卷积算法将是确定的，即默认算法。如果配合上设置 Torch 的随机种子为固定值的话，应该可以保证每次运行网络的时候相同输入的输出是固定的

```python
def same_seeds(seed):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # 为所有GPU都进行种子随机
    np.random.seed(seed)  
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
```

🔵保存和加载模型

```python
torch.save(net.state_dict(), 'mlp.params')
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()
```

🔵`torch.Embedding(num_embeddings, embedding_dim)` 词嵌入

`num_embeddings`表示词汇的数量

`embedding_dim`表示输出词向量的维度。

## matplotlib



## seaborn



## Tensorflow

### 基础使用

构建常量和变量（可以适配numpy）：

```python
a=tf.constant([[1,2],[3,4]])
print(a)
# 变量，trainable表示可用于求导，默认为true
av = tf.Variable(a, name="a var", trainable=True)
print(av)
# 初始化10X10，全部为1的tensor常量
a=tf.constant(1,dtype=tf.float32, shape=[10,10], name='good')
```

### name_scope的使用

会为当前局部代码或者其他命名的变量添加前缀。

```python
with tf.name_scope("demo"):
    a=tf.Variable([1,2], name='a')
    print(a)
a=tf.Variable([1,2], name='a')
print(a)
```

输出：

```
<tf.Variable 'demo/a:0' shape=(2,) dtype=int32, numpy=array([1, 2])>
<tf.Variable 'a:0' shape=(2,) dtype=int32, numpy=array([1, 2])>
```
